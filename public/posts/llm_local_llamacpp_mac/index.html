<!--
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

- -->
<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="shrink-to-fit=no,width=device-width,height=device-height,initial-scale=1,user-scalable=1"><meta name=description content="My corner of the web"><title>Ollama vs LM Studio vs llama.cpp — A No-BS Guide to Running LLMs Locally on macOS</title>
<link rel=author href=/humans.txt><link rel=canonical href=https://adlermedrado.com.br><link href=/css/styles.css rel=stylesheet></head><body><header><nav class=navbar role=navigation><div class=navbar_left><a href=/ class=h-card rel=me><strong>Adler Medrado</strong></a></div><div class=navbar_right><a href=/posts>Posts</a>
<a href=/now>Now</a>
<a href=/tags>Tags</a></div></nav></header><main><section class=section><article><div><h1>Ollama vs LM Studio vs llama.cpp — A No-BS Guide to Running LLMs Locally on macOS</h1><div><div><p><small><time>April 21, 2025</time>
|
3 minutes read</small></p></div><span class=line_break></span></div><div class=content><p>Let’s cut the fluff: if you care about privacy, speed, and having full control over your stack, running LLMs locally is no longer optional — it’s survival. Cloud’s nice until it’s not. Especially when your data is the product and your API bill explodes overnight.</p><p>I put this guide together because I wanted <strong>LLMs running locally</strong>, even with limited hardware — no vendor lock-in, no middlemen sniffing packets, just raw local compute. And yes, I run this stuff daily on a MacBook Air M1 with 16GB RAM. Modest? Yep. Enough? Hell yes.</p><h2 id=-ollama-vs-lm-studio-vs-llamacpp--real-talk>📊 Ollama vs LM Studio vs llama.cpp — Real Talk</h2><table><thead><tr><th>Feature</th><th><strong>Ollama</strong></th><th><strong>LM Studio</strong></th><th><strong>llama.cpp</strong></th></tr></thead><tbody><tr><td><strong>Interface</strong></td><td>CLI + API</td><td>GUI (Electron)</td><td>Pure CLI</td></tr><tr><td><strong>Open Source</strong></td><td>Yes (MIT, server included)</td><td>Nope</td><td>Yes (MIT)</td></tr><tr><td><strong>Quantization</strong></td><td>GGUF via llama.cpp</td><td>GGUF via llama.cpp</td><td>Native GGUF</td></tr><tr><td><strong>Model Install</strong></td><td>One-liner (<code>ollama run</code>)</td><td>Manual (GUI)</td><td>Manual (CLI/scripts)</td></tr><tr><td><strong>Modelfile Support</strong></td><td>Yes (customizable)</td><td>No</td><td>Yes (limited)</td></tr><tr><td><strong>Customization</strong></td><td>Medium</td><td>Basic</td><td>Full control</td></tr><tr><td><strong>Offline</strong></td><td>Yes</td><td>Yes</td><td>Yes</td></tr><tr><td><strong>Local API</strong></td><td>Yes (port 11434)</td><td>Yes (via server)</td><td>Yes (via <code>server</code> binary)</td></tr><tr><td><strong>RAM Usage (idle)</strong></td><td>~1GB</td><td>~2GB</td><td>~100MB</td></tr><tr><td><strong>Complexity</strong></td><td>Low</td><td>Baby mode</td><td>Medium to high</td></tr><tr><td><strong>Prod Ready</strong></td><td>Yes (with tuning)</td><td>No</td><td>Absolutely</td></tr></tbody></table><h2 id=-why-run-it-local>🚀 Why Run It Local?</h2><p>Here’s why you should stop relying on the cloud when it comes to LLMs:</p><ul><li>🔐 <strong>Privacy</strong> — No data leaves your machine. Period.</li><li>⚡ <strong>Low latency</strong> — Local means instant, no round trips.</li><li>💸 <strong>Free</strong> — No monthly API bloodsucking.</li><li>🛠️ <strong>Customizable</strong> — Tweak it all, from tokens to system prompts.</li><li>🔌 <strong>Offline</strong> — No signal? No problem.</li></ul><p>GGUF quantized models are a game changer. You can run 7B models without melting your Mac. It’s not just viable — it’s smooth.</p><h2 id=-setting-up-llamacpp-on-macos-the-fast-way>🔧 Setting Up llama.cpp on macOS (The Fast Way)</h2><h3 id=what-you-need>What you need:</h3><ul><li>macOS 12.6+ (Apple Silicon strongly recommended)</li><li><a href=https://brew.sh/>Homebrew</a></li><li>Xcode Command Line Tools:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>xcode-select --install
</span></span></code></pre></div></li><li>CMake:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install cmake
</span></span></code></pre></div></li></ul><h3 id=build-llamacpp-with-metal-support>Build llama.cpp with Metal support:</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/ggerganov/llama.cpp
</span></span><span style=display:flex><span>cd llama.cpp
</span></span><span style=display:flex><span>mkdir build <span style=color:#f92672>&amp;&amp;</span> cd build
</span></span><span style=display:flex><span>cmake .. -DLLAMA_METAL<span style=color:#f92672>=</span>ON
</span></span><span style=display:flex><span>cmake --build . --config Release -j<span style=color:#66d9ef>$(</span>sysctl -n hw.logicalcpu<span style=color:#66d9ef>)</span>
</span></span></code></pre></div><blockquote><p>🔧 <code>-j$(sysctl -n hw.logicalcpu)</code> uses all your cores — because we don’t waste time.</p></blockquote><h3 id=download-a-model-example-phi-3>Download a model (Example: Phi-3)</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir models <span style=color:#f92672>&amp;&amp;</span> cd models
</span></span><span style=display:flex><span>curl -LO https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf
</span></span><span style=display:flex><span>cd ..
</span></span></code></pre></div><blockquote><p>🧠 Go with <code>Q4_K_M</code> for best balance. Forget float32 unless you like RAM pain.</p></blockquote><h3 id=run-the-damn-thing>Run the damn thing</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./bin/llama-cli -m models/Phi-3-mini-4k-instruct-fp16.gguf -p <span style=color:#e6db74>&#34;Explain recursion in Python.&#34;</span> -n <span style=color:#ae81ff>200</span>
</span></span></code></pre></div><blockquote><p>Need an API?<br>Just do this:<br><code>./bin/server</code> → then hit <code>http://localhost:8080</code></p></blockquote><h2 id=-models-that-actually-work-on-a-mac-m1>🧠 Models That Actually Work on a Mac M1</h2><table><thead><tr><th>Model</th><th>Size</th><th>Use Case</th></tr></thead><tbody><tr><td>Phi-3-mini-4k</td><td>~2.3GB</td><td>General tasks / instruction</td></tr><tr><td>Mistral-7B-Instruct</td><td>~4.2GB</td><td>Chat / coding / workflows</td></tr><tr><td>CodeLlama-7B</td><td>~3.8GB</td><td>Code assistant</td></tr><tr><td>DeepSeek-Coder-6.7B</td><td>~3.5GB</td><td>Dev work / tech-heavy prompts</td></tr></tbody></table><h2 id=-final-words>🧩 Final Words</h2><p>Running LLMs locally isn’t hype — it’s rebellion. It’s freedom.<br><strong>llama.cpp</strong> gives you total control. <strong>Ollama</strong> and <strong>LM Studio</strong>? Handy, but still wrappers.</p><p>You choose the tools. You own the data.<br>No middleman. No leash.</p><hr><br>Want to go deeper? Build your own local RAG pipeline.
Add voice. Add vision. Build like it’s 1999 again — but with 2025 firepower.<p>Welcome to the resistance.</p></div></div></article><div><div><div><a href=https://adlermedrado.com.br/posts/os-lugares-onde-nunca-mais-voltei/>&#8592; Os lugares onde nunca mais voltei, mas que ainda me habitam</a>
|</div></div></div></section></main><span class=line_break></span>
<span class=line_break></span><footer><div><p><br><small><em>Privacy policy: this website employs no tracking.</em></small></p></div><div><p><small>&copy; 1996-2025 Adler Medrado</small></p></div><div class=gpg_signed_info><p>All pages on this website are PGP signed.
Import my <a href=/pub-key.asc>public key</a> and check with <em>curl https://adlermedrado.com.br/posts/llm_local_llamacpp_mac/ | gpg --verify</em></p></div></footer></body></html><!--
-----BEGIN PGP SIGNATURE-----

iQIzBAEBCAAdFiEEB9cP28xEbBnKQwLObV8aePHcNh0FAmgKj5oACgkQbV8aePHc
Nh3kfQ//VX+bpMlZ9agk6hbwWmO6F+h+7w+K2g8Zm5fS3qsCWab3vk2Ssdsq64Q0
HCg/UXlsbNS6xgjHftVpkUon6PhhIPXbJGdRYsiUHXnw+Jxtc0T9xy2xR9FnZ+BS
C0tsDAxifjJQoCu7HhUL5SPKpm+97AENLV2gl9IWc18dbzvOGcdrZYgJ9x5qLT5K
HSzjff0tw7AAwqd8Dvq3P2tN0lYe6H4IGvzRrXusCznPPk0k1S4Sl9y0V6ND2pdZ
ye3nizUw/LUjwxbgJIMaiK+C/S4Tt1h+dYSi7q5HfCIEXmcvjA0TTkWoT/RanpGc
hdNy8iKsc1cUFGF92JfAqP1VivGgip8DUlDOWX+aOLHeGA8pLUiZKV21nCsvrFU9
tiL8LXPt6WPtNo3PQfqhv3IJ6h5d7zkNLNapsS3HowS8zocwUyFSULqrHNb5G8et
P70I7vJpvjkG/Uq36MVPOTklUF4EaKjJu22WMVyr2wfhS386y3R7qXF+hqZB8vmf
ftoYzS67e9sOzyS5hhreAFDB4iliDK4uRlnHhEYuSV3p/aid7vyqnYZRRFSbxNDN
+r3KQekMEZX84/9kXxLHOh1ly19su7P6Ngp3NeotjGy8/rzMp9LFgT03qzcmA9yA
aIi2RZiIuWXyfzxKx4vKVcVTMtntzut6NdK8yw9BnvnZr5V2Tac=
=m0za
-----END PGP SIGNATURE-----
-->
