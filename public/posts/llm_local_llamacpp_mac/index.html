<!--
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

- -->
<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:title" content="Ollama vs LM Studio vs llama.cpp — A No-BS Guide to Running LLMs Locally on macOS"><meta property="og:description" content="Let’s cut the fluff: if you care about privacy, speed, and having full control over your stack, running LLMs locally is no longer optional — it’s survival. Cloud’s nice until it’s not. Especially when your data is the product and your API bill explodes overnight.
I put this guide together because I wanted LLMs running locally, even with limited hardware — no vendor lock-in, no middlemen sniffing packets, just raw local compute. And yes, I run this stuff daily on a MacBook Air M1 with 16GB RAM. Modest? Yep. Enough? Hell yes."><meta property="og:url" content="https://adlermedrado.com.br/posts/llm_local_llamacpp_mac/"><meta property="og:site_name" content="Adler Medrado's corner of the web"><meta property="og:type" content="article"><meta property="og:image" content="/images/default-og-image.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Ollama vs LM Studio vs llama.cpp — A No-BS Guide to Running LLMs Locally on macOS"><meta name=twitter:description content="Let’s cut the fluff: if you care about privacy, speed, and having full control over your stack, running LLMs locally is no longer optional — it’s survival. Cloud’s nice until it’s not. Especially when your data is the product and your API bill explodes overnight.
I put this guide together because I wanted LLMs running locally, even with limited hardware — no vendor lock-in, no middlemen sniffing packets, just raw local compute. And yes, I run this stuff daily on a MacBook Air M1 with 16GB RAM. Modest? Yep. Enough? Hell yes."><meta name=twitter:image content="/images/default-og-image.png"><title>Ollama vs LM Studio vs llama.cpp — A No-BS Guide to Running LLMs Locally on macOS</title>
<meta name=description content="Let’s cut the fluff: if you care about privacy, speed, and having full control over your stack, running LLMs locally is no longer optional — it’s survival. Cloud’s nice until it’s not. Especially when your data is the product and your API bill explodes overnight.
I put this guide together because I wanted LLMs running locally, even with limited hardware — no vendor lock-in, no middlemen sniffing packets, just raw local compute. And yes, I run this stuff daily on a MacBook Air M1 with 16GB RAM. Modest? Yep. Enough? Hell yes."><link rel=author href=/humans.txt><link rel=icon type=image/png href=/images/favicon.png><link rel=canonical href=https://adlermedrado.com.br/posts/llm_local_llamacpp_mac/><link href=/css/styles.css rel=stylesheet></head><body><header class=glitch-zone><nav class=navbar role=navigation aria-label="Main Navigation"><div class=navbar_left><a href=/ class=h-card rel=me><strong>Adler Medrado<span class=cursor-blink>|</span></strong></a></div><div class="navbar_right navbar_right_animated"><a href=/posts>posts</a>
<a href=/now>what am i doing now</a>
<a href=/uses>what am i using</a>
<a href=/tags>tags</a></div></nav></header><main><section class=section><article><div><h1>Ollama vs LM Studio vs llama.cpp — A No-BS Guide to Running LLMs Locally on macOS</h1><div><div><p><small><time>April 21, 2025</time>
|
3 minutes read</small></p></div><span class=line_break></span></div><div class=content><p>Let’s cut the fluff: if you care about privacy, speed, and having full control over your stack, running LLMs locally is no longer optional — it’s survival. Cloud’s nice until it’s not. Especially when your data is the product and your API bill explodes overnight.</p><p>I put this guide together because I wanted <strong>LLMs running locally</strong>, even with limited hardware — no vendor lock-in, no middlemen sniffing packets, just raw local compute. And yes, I run this stuff daily on a MacBook Air M1 with 16GB RAM. Modest? Yep. Enough? Hell yes.</p><h2 id=-ollama-vs-lm-studio-vs-llamacpp--real-talk>📊 Ollama vs LM Studio vs llama.cpp — Real Talk</h2><table><thead><tr><th>Feature</th><th><strong>Ollama</strong></th><th><strong>LM Studio</strong></th><th><strong>llama.cpp</strong></th></tr></thead><tbody><tr><td><strong>Interface</strong></td><td>CLI + API</td><td>GUI (Electron)</td><td>Pure CLI</td></tr><tr><td><strong>Open Source</strong></td><td>Yes (MIT, server included)</td><td>Nope</td><td>Yes (MIT)</td></tr><tr><td><strong>Quantization</strong></td><td>GGUF via llama.cpp</td><td>GGUF via llama.cpp</td><td>Native GGUF</td></tr><tr><td><strong>Model Install</strong></td><td>One-liner (<code>ollama run</code>)</td><td>Manual (GUI)</td><td>Manual (CLI/scripts)</td></tr><tr><td><strong>Modelfile Support</strong></td><td>Yes (customizable)</td><td>No</td><td>Yes (limited)</td></tr><tr><td><strong>Customization</strong></td><td>Medium</td><td>Basic</td><td>Full control</td></tr><tr><td><strong>Offline</strong></td><td>Yes</td><td>Yes</td><td>Yes</td></tr><tr><td><strong>Local API</strong></td><td>Yes (port 11434)</td><td>Yes (via server)</td><td>Yes (via <code>server</code> binary)</td></tr><tr><td><strong>RAM Usage (idle)</strong></td><td>~1GB</td><td>~2GB</td><td>~100MB</td></tr><tr><td><strong>Complexity</strong></td><td>Low</td><td>Baby mode</td><td>Medium to high</td></tr><tr><td><strong>Prod Ready</strong></td><td>Yes (with tuning)</td><td>No</td><td>Absolutely</td></tr></tbody></table><h2 id=-why-run-it-local>🚀 Why Run It Local?</h2><p>Here’s why you should stop relying on the cloud when it comes to LLMs:</p><ul><li>🔐 <strong>Privacy</strong> — No data leaves your machine. Period.</li><li>⚡ <strong>Low latency</strong> — Local means instant, no round trips.</li><li>💸 <strong>Free</strong> — No monthly API bloodsucking.</li><li>🛠️ <strong>Customizable</strong> — Tweak it all, from tokens to system prompts.</li><li>🔌 <strong>Offline</strong> — No signal? No problem.</li></ul><p>GGUF quantized models are a game changer. You can run 7B models without melting your Mac. It’s not just viable — it’s smooth.</p><h2 id=-setting-up-llamacpp-on-macos-the-fast-way>🔧 Setting Up llama.cpp on macOS (The Fast Way)</h2><h3 id=what-you-need>What you need:</h3><ul><li>macOS 12.6+ (Apple Silicon strongly recommended)</li><li><a href=https://brew.sh/>Homebrew</a></li><li>Xcode Command Line Tools:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>xcode-select --install
</span></span></code></pre></div></li><li>CMake:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install cmake
</span></span></code></pre></div></li></ul><h3 id=build-llamacpp-with-metal-support>Build llama.cpp with Metal support:</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/ggerganov/llama.cpp
</span></span><span style=display:flex><span>cd llama.cpp
</span></span><span style=display:flex><span>mkdir build <span style=color:#f92672>&amp;&amp;</span> cd build
</span></span><span style=display:flex><span>cmake .. -DLLAMA_METAL<span style=color:#f92672>=</span>ON
</span></span><span style=display:flex><span>cmake --build . --config Release -j<span style=color:#66d9ef>$(</span>sysctl -n hw.logicalcpu<span style=color:#66d9ef>)</span>
</span></span></code></pre></div><blockquote><p>🔧 <code>-j$(sysctl -n hw.logicalcpu)</code> uses all your cores — because we don’t waste time.</p></blockquote><h3 id=download-a-model-example-phi-3>Download a model (Example: Phi-3)</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir models <span style=color:#f92672>&amp;&amp;</span> cd models
</span></span><span style=display:flex><span>curl -LO https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf
</span></span><span style=display:flex><span>cd ..
</span></span></code></pre></div><blockquote><p>🧠 Go with <code>Q4_K_M</code> for best balance. Forget float32 unless you like RAM pain.</p></blockquote><h3 id=run-the-damn-thing>Run the damn thing</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./bin/llama-cli -m models/Phi-3-mini-4k-instruct-fp16.gguf -p <span style=color:#e6db74>&#34;Explain recursion in Python.&#34;</span> -n <span style=color:#ae81ff>200</span>
</span></span></code></pre></div><blockquote><p>Need an API?<br>Just do this:<br><code>./bin/server</code> → then hit <code>http://localhost:8080</code></p></blockquote><h2 id=-models-that-actually-work-on-a-mac-m1>🧠 Models That Actually Work on a Mac M1</h2><table><thead><tr><th>Model</th><th>Size</th><th>Use Case</th></tr></thead><tbody><tr><td>Phi-3-mini-4k</td><td>~2.3GB</td><td>General tasks / instruction</td></tr><tr><td>Mistral-7B-Instruct</td><td>~4.2GB</td><td>Chat / coding / workflows</td></tr><tr><td>CodeLlama-7B</td><td>~3.8GB</td><td>Code assistant</td></tr><tr><td>DeepSeek-Coder-6.7B</td><td>~3.5GB</td><td>Dev work / tech-heavy prompts</td></tr></tbody></table><h2 id=-final-words>🧩 Final Words</h2><p>Running LLMs locally isn’t hype — it’s rebellion. It’s freedom.<br><strong>llama.cpp</strong> gives you total control. <strong>Ollama</strong> and <strong>LM Studio</strong>? Handy, but still wrappers.</p><p>You choose the tools. You own the data.<br>No middleman. No leash.</p><hr><br>Want to go deeper? Build your own local RAG pipeline.
Add voice. Add vision. Build like it’s 1999 again — but with 2025 firepower.<p>Welcome to the resistance.</p></div></div></article><div><div><div><a href=https://adlermedrado.com.br/posts/os-lugares-onde-nunca-mais-voltei/>&#8592; Os lugares onde nunca mais voltei, mas que ainda me habitam</a>
|
<a href=https://adlermedrado.com.br/posts/gpg-intro-en/>Why You Should Start Using GPG Now &#8594;</a></div></div></div></section></main><span class=line_break></span>
<span class=line_break></span><footer class=glitch-zone><div><p><br><small><em>Privacy policy: this website employs no tracking.</em></small></p></div><div><p><small>&copy; 1996-2025 Adler Medrado</small></p></div><div class=gpg_signed_info><p>All pages on this website are PGP signed.
Import my <a href=/pub-key.asc>public key</a> and check with <em>curl https://adlermedrado.com.br/posts/llm_local_llamacpp_mac/ | gpg --verify</em></p></div></footer></body></html><!--
-----BEGIN PGP SIGNATURE-----

iQIzBAEBCAAdFiEEB9cP28xEbBnKQwLObV8aePHcNh0FAmgfeb0ACgkQbV8aePHc
Nh0d2RAApjdL63C7Lv/kO3G8aTtbUypLaKpyrMjJA8ejSmatC87uNnP5gGEEtY4j
8bzRDL3RfuDlt5sBEOsxE2mAlWKup+BjjhggOaAQKCjCYd8SCm9i5JTZqyUhR6zp
hLlZqFhCrzVpmgRz4pkpxLzaSJQcGt1L2V5XXVJRskSfCT6xnCy6WVLLFvXptFIL
pdqLuQapVLAAIPII6XHIFl8HL8OGpDOcl23CpLOHt0VYkcTdJC170kD+4+fg8OrX
HUMyQ+k785o5o7FfNbqwr8fENskg2tJDblG4XmRWI6IpuqXIQavaIbFl5XznKZkt
rBM/gTJT0ixX+uRqdZUds6nNdm0mJ67oYPf+Ux+qxp/2tHBunwd9KGweecU2TSJM
ptCfrJJFrlDE+OgTFlxWRLmdxHwyieGLIVOhvyx4XQSxJcoUpePTPSBzVOloYC5O
tRo6RIjDMIVYq2sm1KhKtHau3e+jYD608AsyGABeSexT2Q0c4UQsGye+TBUlLQmm
D6gmK5lIdgOkUKDdWN+rWKQBirUb1ge9yI2NsmC6qbe/RSjuG0bYaxX5rDl4eQJ0
arjs3SajKDAKc/5kGfNjehrucON3qjTmuCOVk0x4ETQH76Jq0kMISqxjbTDi1lam
1TQzAkElXYqyBZGUlEyqiAG1VsHCvGq8MRg0u8VPMZTsGMBhfZE=
=7xbU
-----END PGP SIGNATURE-----
-->
