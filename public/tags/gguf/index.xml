<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gguf on Adler Medrado's corner of the web</title><link>https://adlermedrado.com.br/tags/gguf/</link><description>Recent content in Gguf on Adler Medrado's corner of the web</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 21 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://adlermedrado.com.br/tags/gguf/index.xml" rel="self" type="application/rss+xml"/><item><title>Ollama vs LM Studio vs llama.cpp — A No-BS Guide to Running LLMs Locally on macOS</title><link>https://adlermedrado.com.br/posts/llm_local_llamacpp_mac/</link><pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate><guid>https://adlermedrado.com.br/posts/llm_local_llamacpp_mac/</guid><description>&lt;p>Let’s cut the fluff: if you care about privacy, speed, and having full control over your stack, running LLMs locally is no longer optional — it’s survival. Cloud’s nice until it’s not. Especially when your data is the product and your API bill explodes overnight.&lt;/p>
&lt;p>I put this guide together because I wanted &lt;strong>LLMs running locally&lt;/strong>, even with limited hardware — no vendor lock-in, no middlemen sniffing packets, just raw local compute. And yes, I run this stuff daily on a MacBook Air M1 with 16GB RAM. Modest? Yep. Enough? Hell yes.&lt;/p></description></item></channel></rss>