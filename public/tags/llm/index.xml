<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Llm on Adler Medrado's corner of the web</title><link>http://xlqlev6ixhge3wyoivxhpcvuv2v5it2m2dxjnfng5d66jb7c6kpv6qid.onion/tags/llm/</link><description>Recent content in Llm on Adler Medrado's corner of the web</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sat, 14 Jun 2025 14:59:31 -0300</lastBuildDate><atom:link href="http://xlqlev6ixhge3wyoivxhpcvuv2v5it2m2dxjnfng5d66jb7c6kpv6qid.onion/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Building My Own Sovereign RAG for Secure Code Analysis</title><link>http://xlqlev6ixhge3wyoivxhpcvuv2v5it2m2dxjnfng5d66jb7c6kpv6qid.onion/posts/sovereign-rag/</link><pubDate>Sat, 14 Jun 2025 14:59:31 -0300</pubDate><guid>http://xlqlev6ixhge3wyoivxhpcvuv2v5it2m2dxjnfng5d66jb7c6kpv6qid.onion/posts/sovereign-rag/</guid><description>&lt;h2 id="building-my-own-sovereign-rag-for-secure-code-analysis"&gt;Building My Own Sovereign RAG for Secure Code Analysis&lt;/h2&gt;
&lt;p&gt;Lately, I’ve been taking a closer look at some code analysis tools that claim to detect security vulnerabilities in software projects. The idea itself is solid. I got one of these tools recommended to me and decided to dig deeper to see what’s really behind these solutions.&lt;/p&gt;
&lt;p&gt;Pretty quickly I noticed a pattern: these platforms are far from cheap. Some offer limited free plans, but we all know how this game works. When something that good is offered for “free”, the real price usually comes from somewhere else — data collection, vendor lock-in, black-box models processing your code in someone else’s cloud. And since I’ve been deeply studying AI lately, especially Retrieval-Augmented Generation (RAG), the question came naturally: why not build my own pipeline, fully local, sovereign, using open-source tools, running on my own machine, and depending on no one?&lt;/p&gt;</description></item><item><title>Como construí meu próprio RAG soberano para análise de segurança de código</title><link>http://xlqlev6ixhge3wyoivxhpcvuv2v5it2m2dxjnfng5d66jb7c6kpv6qid.onion/posts/rag_soberano/</link><pubDate>Sat, 14 Jun 2025 14:48:35 -0300</pubDate><guid>http://xlqlev6ixhge3wyoivxhpcvuv2v5it2m2dxjnfng5d66jb7c6kpv6qid.onion/posts/rag_soberano/</guid><description>&lt;h2 id="construindo-meu-próprio-rag-soberano-para-análise-de-segurança-de-código"&gt;Construindo meu próprio RAG soberano para análise de segurança de código&lt;/h2&gt;
&lt;p&gt;Nos últimos tempos, comecei a olhar com mais atenção para algumas ferramentas de análise de código que prometem identificar falhas de segurança em projetos. A ideia é boa. Recebi uma dessas ferramentas como sugestão e fui atrás para entender melhor o que havia por trás da proposta.&lt;/p&gt;
&lt;p&gt;Logo de cara percebi um padrão: os preços dessas plataformas não são exatamente convidativos. Algumas até oferecem planos gratuitos limitados, mas a gente sabe como funciona o jogo. Quando algo muito bom aparece “de graça”, o custo real costuma vir de outro lugar. Coleta de dados, lock-in na plataforma, modelos black-box processando seu código na nuvem de terceiros. E como hoje eu venho estudando bastante IA e, em especial, o tema dos RAGs (Retrieval-Augmented Generation), a pergunta veio automática: por que não montar o meu próprio pipeline, 100% local, soberano, usando ferramentas open-source, rodando direto na minha máquina, sem depender de ninguém?&lt;/p&gt;</description></item><item><title>Ollama vs LM Studio vs llama.cpp — A No-BS Guide to Running LLMs Locally on macOS</title><link>http://xlqlev6ixhge3wyoivxhpcvuv2v5it2m2dxjnfng5d66jb7c6kpv6qid.onion/posts/llm_local_llamacpp_mac/</link><pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate><guid>http://xlqlev6ixhge3wyoivxhpcvuv2v5it2m2dxjnfng5d66jb7c6kpv6qid.onion/posts/llm_local_llamacpp_mac/</guid><description>&lt;p&gt;Let’s cut the fluff: if you care about privacy, speed, and having full control over your stack, running LLMs locally is no longer optional — it’s survival. Cloud’s nice until it’s not. Especially when your data is the product and your API bill explodes overnight.&lt;/p&gt;
&lt;p&gt;I put this guide together because I wanted &lt;strong&gt;LLMs running locally&lt;/strong&gt;, even with limited hardware — no vendor lock-in, no middlemen sniffing packets, just raw local compute. And yes, I run this stuff daily on a MacBook Air M1 with 16GB RAM. Modest? Yep. Enough? Hell yes.&lt;/p&gt;</description></item></channel></rss>